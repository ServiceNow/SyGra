mixtral8x7b:
  model_type: tgi
  # URL and auth_token should be defined at .env file as SYGRA_MIXTRAL8X7B_URL and SYGRA_MIXTRAL8X7B_TOKEN
  hf_chat_template_model_id: meta-math/MetaMath-Mistral-7B
  parameters:
    max_new_tokens: 500
    temperature: 1.0

mixtral_instruct_8x22b:
  model_type: vllm
  # URL and auth_token should be defined at .env file as SYGRA_MIXTRAL_INSTRUCT_8X22B_URL and SYGRA_MIXTRAL_INSTRUCT_8X22B_TOKEN
  hf_chat_template_model_id: meta-math/MetaMath-Mistral-7B
  parameters:
    max_tokens: 1024
    temperature: 1.0
    stop: ["</s>", "[INST]"]

mistralai:
  model_type: mistralai
  model: azureai
  # URL and auth_token should be defined at .env file as SYGRA_MISTRALAI_URL and SYGRA_MISTRALAI_TOKEN
  parameters:
    max_tokens: 500
    temperature: 1.0

gpt4:
  model_type: azure_openai
  model: gpt-4-32k
  # URL and auth_token should be defined at .env file as SYGRA_GPT4_URL and SYGRA_GPT4_TOKEN
  api_version: 2024-05-01-preview
  parameters:
    max_tokens: 500
    temperature: 1.0

gpt-4o:
  model_type: azure_openai
  model: gpt-4o
  # URL and auth_token should be defined at .env file as SYGRA_GPT-4O_URL and SYGRA_GPT-4O_TOKEN
  api_version: 2024-02-15-preview
  parameters:
    max_tokens: 500
    temperature: 1.0

gpt-4o-mini:
  model_type: azure_openai
  model: gpt-4o-mini
  # URL and auth_token should be defined at .env file as SYGRA_GPT-4O-MINI_URL and SYGRA_GPT-4O-MINI_TOKEN
  api_version: 2024-08-01-preview
  parameters:
    max_tokens: 5000
    temperature: 0.0001

#QWEN VL 72b deployed in vllm
qwen_vl_72b:
  # URL and auth_token should be defined at .env file as SYGRA_QWEN_VL_72B_URL and SYGRA_QWEN_VL_72B_TOKEN
  hf_chat_template_model_id: Qwen/Qwen2.5-VL-72B-Instruct
  model_type: vllm
  parameters:
    max_tokens: 2048
    temperature: 1.0

#QWEN 32B deployed in vllm
qwen3_32b:
  # URL and auth_token should be defined at .env file as SYGRA_QWEN3_32B_URL and SYGRA_QWEN3_32B_TOKEN
  model_serving_name: qwen3_32b
  hf_chat_template_model_id: Qwen/Qwen3-32B
  model_type: vllm
  parameters:
    max_tokens: 2048
    temperature: 0
#    extra_body:
#      chat_template_kwargs:
#        enable_thinking: false

#qwen3_1.7b
qwen3_1.7b:
  model_type: ollama
  # URL should be defined at .env file as SYGRA_QWEN3_1.7B_URL. auth_token is not needed
  # Default URL for ollama is http://localhost:11434
  hf_chat_template_model_id: Qwen/Qwen3-1.7B
  post_process: sygra.core.models.model_postprocessor.RemoveThinkData
  parameters:
    temperature: 0.8

gpt-audio:
  model_type: azure_openai
  model: gpt-4o-audio-preview
  # URL and auth_token should be defined at .env file as SYGRA_GPT-4O-AUDIO-PREVIEW_URL and SYGRA_GPT-4O-AUDIO-PREVIEW_TOKEN
  api_version: 2025-01-01-preview
  parameters:
    max_tokens: 10240
    temperature: 1.0

# TTS openai model
tts_openai:
  model: tts
  output_type: audio
  model_type: azure_openai
  api_version: 2025-03-01-preview
  # URL and api_key should be defined at .env file as SYGRA_TTS_OPENAI_URL and SYGRA_TTS_OPENAI_TOKEN
  parameters:
    audio:
      voice: "alloy"
      response_format: "wav"

transcribe:
  model: gpt-4o-transcribe
  input_type: audio
  model_type: azure_openai
  api_version: 2025-03-01-preview
  # URL and api_key should be defined at .env file as SYGRA_TRANSCRIBE_URL and SYGRA_TRANSCRIBE_TOKEN
  parameters:
    language: en
    response_format: json
    temperature: 0

# Image generation model
gpt_image_1:
  model: gpt-image-1
  output_type: image
  model_type: azure_openai
  api_version: 2025-04-01-preview
  image_capabilities:
    prompt_char_limit: 32000
    max_edit_images: 16
  # URL and api_key should be defined at .env file as SYGRA_GPT_IMAGE_1_URL and SYGRA_GPT_IMAGE_1_TOKEN
  parameters:
    size: "1024x1024"
    quality: "high"

# Vertex AI model
gemini_2_5_pro:
  model_type: vertex_ai
  model: gemini-2.5-pro
  # Vertex project, location and credentials should be defined at .env file as SYGRA_GEMINI_2_5_PRO_VERTEX_PROJECT, SYGRA_GEMINI_2_5_PRO_VERTEX_LOCATION, SYGRA_GEMINI_2_5_PRO_VERTEX_CREDENTIALS
  parameters:
    max_tokens: 5000
    temperature: 0.5

# Bedrock model
bedrock_model:
  model_type: bedrock
  model: us.anthropic.claude-sonnet-4-5-20250929-v1:0
  # aws_access_key_id, aws_secret_access_key, aws_region_name should be defined at .env file as SYGRA_BEDROCK_MODEL_AWS_ACCESS_KEY_ID, SYGRA_BEDROCK_MODEL_AWS_SECRET_ACCESS_KEY, SYGRA_BEDROCK_MODEL_AWS_REGION_NAME
  parameters:
    max_tokens: 5000
#    temperature: 1
#    reasoning_effort: high
#    thinking:
#      type: enabled
#      budget_tokens: 2048

qwq_32b:
  model_type: vllm
  hf_chat_template_model_id: Qwen/QwQ-32B
  multi_modal: false
  parameters:
   temperature: 0.15
   max_tokens: 128000
   stop: ["<|endoftext|>", "<|im_end|>", "<|eod_id|>"]
   reasoning_effort: high


openai_gpt41:
  model_type: azure_openai
  model: gpt-4.1
  api_version: 2024-02-15-preview
  parameters:
    max_tokens: 5000
    temperature: 0.1

gpt-5:
  model_type: azure_openai
  model: gpt-5
  # URL and auth_token should be defined at .env file as SYGRA_GPT-5_URL and SYGRA_GPT-5_TOKEN
  api_version: 2024-02-15-preview
  parameters:
    max_completion_tokens: 5000
    temperature: 1
    reasoning_effort: high

llama_3_1_405b_instruct:
  model_type: azure
  hf_chat_template_model_id: meta-llama/Meta-Llama-3.1-405B-Instruct
  parameters:
    max_tokens: 4096
    temperature: 0.8

qwen_2.5_32b_vl:
  model_type: vllm
  model_serving_name: qwen_2.5_32b_vl
  hf_chat_template_model_id: Qwen/Qwen2.5-VL-32B-Instruct
#  completions_api: true
  parameters:
    temperature: 0
    max_tokens: 5000
    stop: ["<|endoftext|>", "<|im_end|>", "<|eod_id|>"]

gpt-oss-120b:
  model_type: vllm
  hf_chat_template_model_id: openai/gpt-oss-120b
  parameters:
    temperature: 0.15
    max_tokens: 128000

apriel_1p6_15b_thinker:
  model_serving_name: Apriel-1p6-15B-Thinker
  model_type: vllm
  parameters:
    temperature: 0.0
    max_tokens: 150000
    "stop": ["<|end|>", "[END FINAL RESPONSE]"]

gpt-oss-20b:
  model_type: vllm
  hf_chat_template_model_id: openai/gpt-oss-20b
  parameters:
    temperature: 0.15
    max_tokens: 128000

eval_model:
  model_type: bedrock
  model: us.anthropic.claude-sonnet-4-5-20250929-v1:0
#  api_version: 2024-08-01-preview
  parameters:
    temperature: 0.1

claude_large:
  model_type: claude_proxy
  client_type: http
  backend: proxy
  additional_params:
    thinking:
      type: disabled
    anthropic_beta:
      - computer-use-2025-01-24
  parameters:
    maxTokens: 2500
    temperature: 0

gemini_2.5_proxy:
  model_type: gemini_proxy
  client_type: http
  backend: proxy
  parameters:
    maxOutputTokens: 2500
    temperature: 0
